\section{Compact analysis objects}
CMS and ATLAS experiments are transitioning towards more compact datasets for analysis with event sizes in the order of the kB/event. This implies that a full analysis datasets will be close to 1PB per year taking CMS numbers on their compact analysis objects (nanoAOD), to be compared with the 50PB per year for older analysis objects (miniAOD), these sizes has been estimated taking as a reference LHC delivery of 80 billion events/year (data) and the production of 160 billion events/year (MC) together with expected sizes for the different data types of 7.4MB(RAW), 2.0MB(AOD), 200kB(miniAOD) and 4kB(nanoAOD). These compact objects open the window to evaluate new ways of doing computing and offer different options for the sites currently providing computing and storage resources. In particular storage has been identified as the main challenge for HL-LHC due to the increasing use of disk volumes, and also the costs from the site perspective to operate and maintain complex storage systems.\\
One of the goals of the working group is to study the feasibility to exploit these new analysis objects and its reduction in size to promote less demanding storage at the sites (e.g. stateless storage) while fostering the efforts towards computing resources (CPU, GPU, ML, etc.) with the possibility to access the full analysis objects from a centralized storage through caches to minimize latency and increase file re-usability at site or even at federation/regional level. At this point in time there is the clear need of the engagement from the physics community to converge using these new compact objects and need to be emphasized that the overall goal is to be able to maximise the physics potential of the machine and hence taking as much data as possible which means to maximise the use and efficiency of the global storage resources. Use of storage is a delicate concept, a full storage does not mean it is used in an efficient way.\\
